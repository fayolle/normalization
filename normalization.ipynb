{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rvachev_normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+/+XyuP9XUKxVW4kQnp+E"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB7xwJzdIzwk"
      },
      "source": [
        "Experiments with Rvachev and Taubin normalization. PyTorch is used for computing the derivatives. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7m44KJkqUEx"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "from math import factorial\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.mlab as mlab\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd8tVqrPqVSV"
      },
      "source": [
        "def gradient_unsafe(y, x, grad_outputs=None):\n",
        "  if grad_outputs is None:\n",
        "    grad_outputs = torch.ones_like(y)\n",
        "  grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "  return grad"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNmXONkEB1Ex"
      },
      "source": [
        "def gradient_safe(y, x, grad_outputs=None):\n",
        "  if grad_outputs is None:\n",
        "    grad_outputs = torch.ones_like(y)\n",
        "  grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
        "  grad = torch.nan_to_num(grad)\n",
        "  return grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oU47PH1TB4si"
      },
      "source": [
        "gradient = gradient_safe"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wDYtGsoqaj6"
      },
      "source": [
        "def ellipse(p):\n",
        "  x = p[:,0]\n",
        "  y = p[:,1]\n",
        "  d = 1.0 - (x / 5.0)**2 - (y / 2.0)**2\n",
        "  return d\n",
        "\n",
        "def thin_ellipse(p):\n",
        "  x = p[:,0]\n",
        "  y = p[:,1]\n",
        "  d = 1.0 - (x / 16.9)**2 - (y / 0.5)**2\n",
        "  return d"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR95jwza3-Il"
      },
      "source": [
        "# Create a 2D grid as a torch tensor\n",
        "def torch_grid(xmin, xmax, ymin, ymax, resx=64, resy=64, device='cpu'):\n",
        "  dx = xmax - xmin\n",
        "  dy = ymax - ymin\n",
        "\n",
        "  ed = 0.1*math.sqrt(dx*dx+dy*dy)\n",
        "\n",
        "  x = torch.arange(xmin-ed, xmax+ed, step=(dx+2*ed)/float(resx))\n",
        "  y = torch.arange(ymin-ed, ymax+ed, step=(dy+2*ed)/float(resy))\n",
        "\n",
        "  xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
        "\n",
        "  return xx.to(device), yy.to(device)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jVMNYI-4SXm"
      },
      "source": [
        "# Sample function f() on torch 2D grid x,y\n",
        "def torch_sampling(f, x, y, device='cpu'):\n",
        "  nx = x.shape[0]\n",
        "  ny = x.shape[1]\n",
        "  d = nx * ny\n",
        "  xy = torch.stack((x, y), dim=-1).reshape(d, 2)  \n",
        "  z = f(xy)\n",
        "  z = torch.reshape(z, (nx,ny))\n",
        "  return z"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOwpX99H4m7U"
      },
      "source": [
        "def show_contour_plot(x, y, f): \n",
        "  xx = x.detach().numpy()\n",
        "  yy = y.detach().numpy()\n",
        "  ff = f.detach().numpy()\n",
        "  plt.figure(figsize=[12.8,9.6], frameon=False)\n",
        "  h = plt.contourf(xx, yy, ff)\n",
        "  h.ax.axis('equal')\n",
        "  h.ax.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def show_contour_lines(x, y, f, fname=''):\n",
        "  xx = x.detach().numpy()\n",
        "  yy = y.detach().numpy()\n",
        "  ff = f.detach().numpy()\n",
        "  plt.figure(figsize=[12.8,9.6], frameon=False)\n",
        "  levels = np.arange(-0.5, 0.5, 0.1)\n",
        "  CS = plt.contour(xx, yy, ff, levels)\n",
        "  # CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "  plt.axis('equal')\n",
        "  plt.axis('off')\n",
        "  if fname!='':\n",
        "    plt.savefig(fname)\n",
        "  plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMQMHYW1q54B"
      },
      "source": [
        "NX = 64\n",
        "NY = 64\n",
        "XMIN = 0.0\n",
        "XMAX = 7.0\n",
        "YMIN = -4.0\n",
        "YMAX = 4.0\n",
        "\n",
        "x,y = torch_grid(XMIN,XMAX,YMIN,YMAX,NX,NY)\n",
        "f = torch_sampling(ellipse, x, y)\n",
        "show_contour_lines(x, y, f, fname='ellipse.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGXhg5s2qkwy"
      },
      "source": [
        "def normalize_Rvachev(f, x, n):\n",
        "  fx = f(x)\n",
        "  gradf = gradient(fx, x)\n",
        "  norm_gradf2 = gradf[:,0]**2 + gradf[:,1]**2\n",
        "  norm_gradf = torch.sqrt(norm_gradf2)\n",
        "  #normal = gradf / norm_gradf\n",
        "  normal = torch.zeros(gradf.shape)\n",
        "  normal[:,0] = gradf[:,0] / norm_gradf\n",
        "  normal[:,1] = gradf[:,1] / norm_gradf\n",
        "  # pre-compute w1 (first order normalization of f)\n",
        "  w1 = fx / torch.sqrt(fx**2 + norm_gradf2)\n",
        "  return normalize_Rvachev_(w1, x, normal, n)\n",
        "\n",
        "def normalize_Rvachev_(w1, x, normal, n):\n",
        "  if n == 1:\n",
        "    return w1\n",
        "  else:\n",
        "    temp = normalize_Rvachev_(w1, x, normal, n-1)\n",
        "    return temp - 1.0/factorial(n) * w1**n * directional_derivative(temp, x, normal, n)\n",
        "\n",
        "def directional_derivative(fx, x, normal, n):\n",
        "  if n==1:\n",
        "    gradf = gradient(fx, x)\n",
        "    return gradf[:,0] * normal[:,0] + gradf[:,1] * normal[:,1]\n",
        "  else:\n",
        "    gx = directional_derivative(fx, x, normal, n-1)\n",
        "    gradg = gradient(gx, x)\n",
        "    return gradg[:,0]*normal[:,0] + gradg[:,1]*normal[:,1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUscsXWFIYXv"
      },
      "source": [
        "It seems to explode when trying to normalize with order k > 2. \n",
        "The reason is likely that I am using PyTorch, which uses reverse automatic differentiation. Probably, it is better to use Jax and forward automatic differentiation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7mT8Ipy9PvM"
      },
      "source": [
        "NX = 64\n",
        "NY = 64\n",
        "XMIN = 0.0\n",
        "XMAX = 7.0\n",
        "YMIN = -4.0\n",
        "YMAX = 4.0\n",
        "k = 1 \n",
        "\n",
        "x,y = torch_grid(XMIN,XMAX,YMIN,YMAX,NX,NY)\n",
        "x.requires_grad = True\n",
        "y.requires_grad = True\n",
        "\n",
        "wk = lambda p: normalize_Rvachev(ellipse, p, k)\n",
        "\n",
        "#x = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
        "#wkx = wk(x)\n",
        "#print(wkx)\n",
        "\n",
        "f = torch_sampling(wk, x, y)\n",
        "show_contour_lines(x, y, f, fname='ellipse_Rvachev.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_Taubin(f, x):\n",
        "    \"\"\"Implements the Taubin normalization. \n",
        "    Implements the Taubin normalization of the implicit surface f() at the \n",
        "    point x. \n",
        "    x is assumed to be a Torch tensor for which derivatives are tracked. \n",
        "    \"\"\"\n",
        "\n",
        "    fx = f(x)\n",
        "    gradf = gradient(fx, x)\n",
        "    norm_gradf2 = gradf[:, 0]**2 + gradf[:, 1]**2\n",
        "    norm_gradf = torch.sqrt(norm_gradf2)\n",
        "    d1 = fx / norm_gradf\n",
        "    return d1"
      ],
      "metadata": {
        "id": "sTs8wOAtrXbH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NX = 64\n",
        "NY = 64\n",
        "XMIN = 0.0\n",
        "XMAX = 7.0\n",
        "YMIN = -4.0\n",
        "YMAX = 4.0\n",
        "\n",
        "x,y = torch_grid(XMIN,XMAX,YMIN,YMAX,NX,NY)\n",
        "x.requires_grad = True\n",
        "y.requires_grad = True\n",
        "\n",
        "delta = lambda p: normalize_Taubin(ellipse, p)\n",
        "\n",
        "f = torch_sampling(delta, x, y)\n",
        "show_contour_lines(x, y, f, fname='ellipse_Taubin.png')"
      ],
      "metadata": {
        "id": "bpj4IK9Orl-X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}